{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hurlink/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression as Lin_Reg\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as colors\n",
    "import scipy as sp\n",
    "from sklearn.linear_model import RidgeCV, Ridge, LassoCV, LassoLars, LassoLarsCV, Lasso, ElasticNet, ElasticNetCV, BayesianRidge\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.constraints import max_norm\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(13)\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load training and test sets (assumes you have these in current working directory)\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Feat 1</th>\n",
       "      <th>Feat 2</th>\n",
       "      <th>Feat 3</th>\n",
       "      <th>Feat 4</th>\n",
       "      <th>Feat 5</th>\n",
       "      <th>Feat 6</th>\n",
       "      <th>Feat 7</th>\n",
       "      <th>Feat 8</th>\n",
       "      <th>Feat 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Feat 243</th>\n",
       "      <th>Feat 244</th>\n",
       "      <th>Feat 245</th>\n",
       "      <th>Feat 246</th>\n",
       "      <th>Feat 247</th>\n",
       "      <th>Feat 248</th>\n",
       "      <th>Feat 249</th>\n",
       "      <th>Feat 250</th>\n",
       "      <th>Feat 251</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.998952</td>\n",
       "      <td>0.174118</td>\n",
       "      <td>0.999211</td>\n",
       "      <td>0.996460</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.612863</td>\n",
       "      <td>0.026812</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.217791</td>\n",
       "      <td>0.233629</td>\n",
       "      <td>0.540962</td>\n",
       "      <td>0.901355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.999445</td>\n",
       "      <td>0.174118</td>\n",
       "      <td>0.999329</td>\n",
       "      <td>0.997079</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.688941</td>\n",
       "      <td>0.075030</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.246119</td>\n",
       "      <td>0.143860</td>\n",
       "      <td>0.525384</td>\n",
       "      <td>0.913550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.998759</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.997260</td>\n",
       "      <td>0.996325</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>0.436279</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.119091</td>\n",
       "      <td>0.162869</td>\n",
       "      <td>0.361124</td>\n",
       "      <td>0.884824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.999619</td>\n",
       "      <td>0.174118</td>\n",
       "      <td>0.997969</td>\n",
       "      <td>0.997321</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.709647</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>0.513</td>\n",
       "      <td>0.392743</td>\n",
       "      <td>0.377302</td>\n",
       "      <td>0.613776</td>\n",
       "      <td>0.977236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.998278</td>\n",
       "      <td>0.174118</td>\n",
       "      <td>0.998427</td>\n",
       "      <td>0.996269</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.364235</td>\n",
       "      <td>0.041818</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.096297</td>\n",
       "      <td>0.166459</td>\n",
       "      <td>0.408322</td>\n",
       "      <td>0.921138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 253 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id    Feat 1    Feat 2    Feat 3    Feat 4    Feat 5    Feat 6  Feat 7  \\\n",
       "0   1  0.998952  0.174118  0.999211  0.996460  0.133333  0.057143   0.000   \n",
       "1   2  0.999445  0.174118  0.999329  0.997079  0.133333  0.000000   0.000   \n",
       "2   3  0.998759  0.000000  0.997260  0.996325  0.000000  0.085714   0.125   \n",
       "3   4  0.999619  0.174118  0.997969  0.997321  0.266667  0.057143   0.125   \n",
       "4   5  0.998278  0.174118  0.998427  0.996269  0.200000  0.000000   0.000   \n",
       "\n",
       "   Feat 8  Feat 9    ...     Feat 243  Feat 244  Feat 245  Feat 246  Feat 247  \\\n",
       "0     0.0     0.0    ...          0.0       0.0         0  0.612863  0.026812   \n",
       "1     0.0     0.0    ...          0.0       0.0         0  0.688941  0.075030   \n",
       "2     0.0     0.0    ...          0.0       0.0         0  0.156863  0.436279   \n",
       "3     0.0     0.0    ...          0.0       0.0         0  0.709647  0.075472   \n",
       "4     0.0     0.0    ...          0.0       0.0         0  0.364235  0.041818   \n",
       "\n",
       "   Feat 248  Feat 249  Feat 250  Feat 251    Target  \n",
       "0     0.522  0.217791  0.233629  0.540962  0.901355  \n",
       "1     0.704  0.246119  0.143860  0.525384  0.913550  \n",
       "2     0.000  0.119091  0.162869  0.361124  0.884824  \n",
       "3     0.513  0.392743  0.377302  0.613776  0.977236  \n",
       "4     0.200  0.096297  0.166459  0.408322  0.921138  \n",
       "\n",
       "[5 rows x 253 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect training set\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Feat 1</th>\n",
       "      <th>Feat 2</th>\n",
       "      <th>Feat 3</th>\n",
       "      <th>Feat 4</th>\n",
       "      <th>Feat 5</th>\n",
       "      <th>Feat 6</th>\n",
       "      <th>Feat 7</th>\n",
       "      <th>Feat 8</th>\n",
       "      <th>Feat 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Feat 242</th>\n",
       "      <th>Feat 243</th>\n",
       "      <th>Feat 244</th>\n",
       "      <th>Feat 245</th>\n",
       "      <th>Feat 246</th>\n",
       "      <th>Feat 247</th>\n",
       "      <th>Feat 248</th>\n",
       "      <th>Feat 249</th>\n",
       "      <th>Feat 250</th>\n",
       "      <th>Feat 251</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.999849</td>\n",
       "      <td>0.174118</td>\n",
       "      <td>0.999819</td>\n",
       "      <td>0.997841</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.728471</td>\n",
       "      <td>0.054397</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.416164</td>\n",
       "      <td>0.053998</td>\n",
       "      <td>0.667391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.999958</td>\n",
       "      <td>0.164706</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996741</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.497255</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.165514</td>\n",
       "      <td>0.101973</td>\n",
       "      <td>0.506650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.999666</td>\n",
       "      <td>0.174118</td>\n",
       "      <td>0.999479</td>\n",
       "      <td>0.997376</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.688941</td>\n",
       "      <td>0.019309</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.192069</td>\n",
       "      <td>0.120700</td>\n",
       "      <td>0.498784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.999735</td>\n",
       "      <td>0.174118</td>\n",
       "      <td>0.999655</td>\n",
       "      <td>0.997173</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.654118</td>\n",
       "      <td>0.019089</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.451252</td>\n",
       "      <td>0.164180</td>\n",
       "      <td>0.774466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.999806</td>\n",
       "      <td>0.164706</td>\n",
       "      <td>0.999551</td>\n",
       "      <td>0.997234</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.160433</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.147407</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.481240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 252 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id    Feat 1    Feat 2    Feat 3    Feat 4    Feat 5  Feat 6  Feat 7  \\\n",
       "0   1  0.999849  0.174118  0.999819  0.997841  0.133333     0.2     0.0   \n",
       "1   2  0.999958  0.164706  1.000000  0.996741  0.066667     0.0     0.0   \n",
       "2   3  0.999666  0.174118  0.999479  0.997376  0.000000     0.0     0.0   \n",
       "3   4  0.999735  0.174118  0.999655  0.997173  0.133333     0.0     0.0   \n",
       "4   5  0.999806  0.164706  0.999551  0.997234  0.000000     0.0     0.0   \n",
       "\n",
       "   Feat 8    Feat 9    ...     Feat 242  Feat 243  Feat 244  Feat 245  \\\n",
       "0     0.0  0.000000    ...          0.0       0.0       0.0         0   \n",
       "1     0.0  0.000000    ...          0.0       0.0       0.0         0   \n",
       "2     0.0  0.000000    ...          0.0       0.0       0.0         0   \n",
       "3     0.0  0.363636    ...          0.0       0.0       0.0         0   \n",
       "4     0.0  0.000000    ...          0.0       0.0       0.0         0   \n",
       "\n",
       "   Feat 246  Feat 247  Feat 248  Feat 249  Feat 250  Feat 251  \n",
       "0  0.728471  0.054397     0.649  0.416164  0.053998  0.667391  \n",
       "1  0.497255  0.037736     0.375  0.165514  0.101973  0.506650  \n",
       "2  0.688941  0.019309     1.000  0.192069  0.120700  0.498784  \n",
       "3  0.654118  0.019089     0.333  0.451252  0.164180  0.774466  \n",
       "4  0.627451  0.160433     0.882  0.147407  0.000000  0.481240  \n",
       "\n",
       "[5 rows x 252 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect test set\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnIPezCxzDnozs8w56M3MMuegNzPL3P8HLjMO5J7gfwcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10191cd68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Explore distribution of target\n",
    "plt.hist(train['Target'], bins = 100)\n",
    "plt.title(\"Distribution of Dependent Variable\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split training set into X and y (removing first column containing IDs)\n",
    "X_train = train.iloc[:, 1:-1]\n",
    "y_train = train.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to compute RMSE\n",
    "def scoreRMSE(predictor, X, true_y):\n",
    "    predictions = predictor.predict(X)\n",
    "    return np.sqrt(mean_squared_error(predictions, true_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feat 1</th>\n",
       "      <th>Feat 2</th>\n",
       "      <th>Feat 3</th>\n",
       "      <th>Feat 4</th>\n",
       "      <th>Feat 5</th>\n",
       "      <th>Feat 6</th>\n",
       "      <th>Feat 7</th>\n",
       "      <th>Feat 8</th>\n",
       "      <th>Feat 9</th>\n",
       "      <th>Feat 10</th>\n",
       "      <th>...</th>\n",
       "      <th>Feat 242</th>\n",
       "      <th>Feat 243</th>\n",
       "      <th>Feat 244</th>\n",
       "      <th>Feat 245</th>\n",
       "      <th>Feat 246</th>\n",
       "      <th>Feat 247</th>\n",
       "      <th>Feat 248</th>\n",
       "      <th>Feat 249</th>\n",
       "      <th>Feat 250</th>\n",
       "      <th>Feat 251</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999849</td>\n",
       "      <td>0.174118</td>\n",
       "      <td>0.999819</td>\n",
       "      <td>0.997841</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.728471</td>\n",
       "      <td>0.054397</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.416164</td>\n",
       "      <td>0.053998</td>\n",
       "      <td>0.667391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.999958</td>\n",
       "      <td>0.164706</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996741</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.497255</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.165514</td>\n",
       "      <td>0.101973</td>\n",
       "      <td>0.506650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.999666</td>\n",
       "      <td>0.174118</td>\n",
       "      <td>0.999479</td>\n",
       "      <td>0.997376</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.688941</td>\n",
       "      <td>0.019309</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.192069</td>\n",
       "      <td>0.120700</td>\n",
       "      <td>0.498784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.999735</td>\n",
       "      <td>0.174118</td>\n",
       "      <td>0.999655</td>\n",
       "      <td>0.997173</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.654118</td>\n",
       "      <td>0.019089</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.451252</td>\n",
       "      <td>0.164180</td>\n",
       "      <td>0.774466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.999806</td>\n",
       "      <td>0.164706</td>\n",
       "      <td>0.999551</td>\n",
       "      <td>0.997234</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.160433</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.147407</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.481240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 251 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Feat 1    Feat 2    Feat 3    Feat 4    Feat 5  Feat 6  Feat 7  Feat 8  \\\n",
       "0  0.999849  0.174118  0.999819  0.997841  0.133333     0.2     0.0     0.0   \n",
       "1  0.999958  0.164706  1.000000  0.996741  0.066667     0.0     0.0     0.0   \n",
       "2  0.999666  0.174118  0.999479  0.997376  0.000000     0.0     0.0     0.0   \n",
       "3  0.999735  0.174118  0.999655  0.997173  0.133333     0.0     0.0     0.0   \n",
       "4  0.999806  0.164706  0.999551  0.997234  0.000000     0.0     0.0     0.0   \n",
       "\n",
       "     Feat 9   Feat 10    ...     Feat 242  Feat 243  Feat 244  Feat 245  \\\n",
       "0  0.000000  0.000000    ...          0.0       0.0       0.0         0   \n",
       "1  0.000000  0.000000    ...          0.0       0.0       0.0         0   \n",
       "2  0.000000  0.000000    ...          0.0       0.0       0.0         0   \n",
       "3  0.363636  0.166667    ...          0.0       0.0       0.0         0   \n",
       "4  0.000000  0.000000    ...          0.0       0.0       0.0         0   \n",
       "\n",
       "   Feat 246  Feat 247  Feat 248  Feat 249  Feat 250  Feat 251  \n",
       "0  0.728471  0.054397     0.649  0.416164  0.053998  0.667391  \n",
       "1  0.497255  0.037736     0.375  0.165514  0.101973  0.506650  \n",
       "2  0.688941  0.019309     1.000  0.192069  0.120700  0.498784  \n",
       "3  0.654118  0.019089     0.333  0.451252  0.164180  0.774466  \n",
       "4  0.627451  0.160433     0.882  0.147407  0.000000  0.481240  \n",
       "\n",
       "[5 rows x 251 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove first column to make predictions\n",
    "X_test = test.iloc[:, 1:]\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: remove features that add no value\n",
    "def remove_single_feature(X_train, X_test):\n",
    "    cols = X_train.columns\n",
    "    single_val = []\n",
    "    for index in cols:\n",
    "        if (len(X_train[index].unique()) == 1):\n",
    "            single_val.append(index)\n",
    "\n",
    "    test_val = []\n",
    "    for index in single_val:\n",
    "        if (len(X_test[index].unique()) == 1):\n",
    "            test_val.append(index)\n",
    "            \n",
    "    single_feature = list(set(single_val).intersection(set(test_val)))\n",
    "    \n",
    "    X_single_train = X_train.drop(columns=single_feature)\n",
    "    X_single_test = X_test.drop(columns=single_feature)\n",
    "\n",
    "    return X_single_train, X_single_test, single_feature\n",
    "\n",
    "# Extension on option 1: log small values of features\n",
    "def log_small(X_train, X_test):\n",
    "    X_single_train, X_single_test, single_feature = remove_single_feature(X_train, X_test)\n",
    "    \n",
    "    small_cols = []\n",
    "    for i in X_single_train.columns:\n",
    "        count = sum(X_single_train[i] < 0.1)\n",
    "        if count > .9*5331:\n",
    "            small_cols.append(i)\n",
    "\n",
    "    log_columns = list(set(small_cols).difference(set(single_feature)))\n",
    "\n",
    "    epsilon_X_train = X_single_train.replace(0, 10 ** (-100))\n",
    "    epsilon_X_test = X_single_test.replace(0, 10 ** (-100))\n",
    "    for col in log_columns:\n",
    "        epsilon_X_train[col] = epsilon_X_train[col].apply(np.log)\n",
    "        epsilon_X_test[col] = epsilon_X_test[col].apply(np.log)\n",
    "    return epsilon_X_train, epsilon_X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_scale_pca, X_test_scale_pca = scale_and_pca(X_train, X_test)\n",
    "pd.DataFrame(X_train_scale_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Option 2: PCA\n",
    "def scale_and_pca(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train.values)\n",
    "    X_train_scale = scaler.transform(X_train.values)\n",
    "    X_test_scale = scaler.transform(X_test.values)\n",
    "    \n",
    "    pca = PCA(0.99)\n",
    "    \n",
    "    pca.fit(X_train_scale)\n",
    "    \n",
    "    X_train_scale_pca = pca.transform(X_train_scale)\n",
    "    X_test_scale_pca = pca.transform(X_test_scale)\n",
    "    \n",
    "    return X_train_scale_pca, X_test_scale_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Option 3: Remove highly correlated features\n",
    "def remove_high_correlation(X_train, X_test):\n",
    "    corr_matrix = X_train.corr().abs()\n",
    "    high_corr_var=np.where(corr_matrix>0.999)\n",
    "    high_corr_var=[(corr_matrix.columns[x],corr_matrix.columns[y]) for x,y in zip(*high_corr_var) if x!=y and x<y]\n",
    "    \n",
    "    remove_vars = [i[0] for i in high_corr_var]\n",
    "    print(remove_vars)\n",
    "    \n",
    "    X_corr_train = X_train.drop(columns=remove_vars)\n",
    "    X_corr_test = X_test.drop(columns=remove_vars)\n",
    "    return X_corr_train, X_corr_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_single_train, X_single_test, single_feature = remove_single_feature(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Search for best parameters\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "clf = lgb.LGBMRegressor(objective='regression',\n",
    "                                  learning_rate = 0.01,\n",
    "                                  max_bin = 30, bagging_freq = 5,\n",
    "                                  feature_fraction_seed=13, bagging_seed=13,\n",
    "                                  min_data_in_leaf =6, min_sum_hessian_in_leaf = 11, max_depth = 8)\n",
    "\n",
    "# use a full grid over all parameters\n",
    "param_dist = {\"num_leaves\": [31,36,41,46,51,56,61,63],\n",
    "              \"feature_fraction\": [0.65, 0.7, 0.75, 0.8, 0.85, 0.9],\n",
    "              \"n_estimators\": [100, 200, 300],\n",
    "              \"bagging_fraction\": [0.65, 0.7, 0.75, 0.8, 0.85, 0.9]\n",
    "             }\n",
    "\n",
    "n_iter_search = 20\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search, cv=10)\n",
    "\n",
    "random_search.fit(X_single_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaves = random_search.best_params_['num_leaves']\n",
    "feature = random_search.best_params_['feature_fraction']\n",
    "est = random_search.best_params_['n_estimators']\n",
    "bag = random_search.best_params_['bagging_fraction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(leaves)\n",
    "print(feature)\n",
    "print(est)\n",
    "print(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LGBM model\n",
    "n_folds = 10\n",
    "\n",
    "def rmsle_cv_single(model, X, Y):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=97).get_n_splits(X)\n",
    "    rmse= np.sqrt(-cross_val_score(model, X, Y, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)\n",
    "\n",
    "def train_lgb(X_train, X_test, y):\n",
    "    model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=51,\n",
    "                                  learning_rate = 0.01, n_estimators=300,\n",
    "                                  max_bin = 30, bagging_fraction = 0.65,\n",
    "                                  bagging_freq = 5, feature_fraction = 0.65,\n",
    "                                  feature_fraction_seed=13, bagging_seed=13,\n",
    "                                  min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n",
    "    score = rmsle_cv_single(model_lgb, X_train, y_train)\n",
    "    print(\"lightgbm score: \", score.mean(), \"(\", score.std(), \")\")\n",
    "    model_lgb.fit(X_train, y_train)\n",
    "    lgb_train_pred = model_lgb.predict(X_test)\n",
    "    return lgb_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = train_lgb(X_single_train, X_single_test, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format predictions to be compatible with Kaggle upload\n",
    "sample_submission = pd.DataFrame(data=predictions, columns=['Predicted'])\n",
    "sample_submission.insert(0, \"Id\", range(1, 1 + X_test.shape[0]))\n",
    "sample_submission['Id'] = sample_submission['Id'].astype(str)\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save predictions to .csv file for upload to Kaggle\n",
    "sample_submission.to_csv(\"Single_feature_Random_search_submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
